{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e882c857",
   "metadata": {},
   "source": [
    "## **Reranking based RAG** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08efe877",
   "metadata": {},
   "source": [
    "_**What are your expectations for Fusion Retrieval with the provided manual?**_<br>\n",
    "I do not expect the responses to be far superior than the vanilla approach due to the main reason being that these LLMs still require context and in-depth domain knowledge to truly understand how to rank the final set of chunks or documents appropriately. However, depending on how good the LLM is (in terms of size and amount of data that it was trained on), the results will vary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8584f5",
   "metadata": {},
   "source": [
    "**_How do you plan to test and compare these techniques?_**<br><br>\n",
    "<img src=\"./reranking_workflow.png\" alt=\"Flowchart\" width=\"700\" /><br><br>\n",
    "The approach is pretty straight forward here and the main area where the changes take place compared to vanilla RAG is at the last stage where an LLM is prompted to retrieve the top-L indexes from the final retrieved set of chunks. Teh workflow is as follows:<br>\n",
    "1. The document data is extracted, specifically text and tabular data. \n",
    "2. These data are stored in a way where the sequence is maintained, that way there will be more context for a certain text that may have a table before or after it. \n",
    "3. This set is chunked and converted to BERT based vector embeddings. BERT was chosen because from what I know, they can represent these chunks in a very context aware fashion, thanks to the utilization os the encoder of a transformers. \n",
    "4. Now the query is also converted to BERT based embeddings and using cosine similarity, the top-K chunks are retrieved.\n",
    "5. Now these chunks, along with the query and chunk IDs is passed into an LLM for further ranking. \n",
    "6. The LLM returns a list of IDs of the top-L relevant chunks (L < K). \n",
    "7. The chunks are retrieved from the original set of chunks using these L indexes.\n",
    "8. This set of chunks, which acts as context, along with the query, is passed into a new LLM for a refined information retrieval.\n",
    "\n",
    "**It must be noted that K > L. In this implementation, they are set as 20 and 5 respectively.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32b26d5",
   "metadata": {},
   "source": [
    "_**Comparison Strategy**_<br>\n",
    "There are possibly two main ways in which we can compare this approach with the Reranking apprach. One is by assessing the top-L retrieved chunks and the other is obviously by assessing the final response from the LLM. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b976e13",
   "metadata": {},
   "source": [
    "_**Note**: Considering images is important in order to create a robust RAG system. However, due to technical/financial constraints, images are omitted for this implementation. However, in the absence of such constraints, what I would have done is have the LLM read the image and prompt it to generate a description. This description will be added into the resulting array while also maintaining the sequence. One obvious question in that case will be whether or not the LLM knows about the content in the image, provided that it is very domain specific and unfamiliar to the LLM. One way I thought of on mitigating this issue is by providing some set of surrounding context of the image to the LLM along with the image itself for it to draw better conclusions. These contexts can be the nearest 2 or 3 elements (text, table or another image) surrounding the image in hand. Let this value be J. So, if J is 3, we feed 3 elements before the image and 3 elements after the image as context for the LLM to generate proper a description of the image in hand. This might not be the most robust solution, but there can be scenarios where this will work._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1d8769",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b23167",
   "metadata": {},
   "source": [
    "#### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1bdf0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz # for text extraction\n",
    "import camelot # for table extraction\n",
    "from pathlib import Path\n",
    "from sentence_transformers import SentenceTransformer, util # for semantic vector embedding creation \n",
    "import numpy as np\n",
    "from groq import Groq\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import re\n",
    "import ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdcafcb5",
   "metadata": {},
   "source": [
    "#### 1. Function to extract texts & tables from PDF\n",
    "The goal is to preserve the sequence, that way there will be more context for a certain text that may have a table before or after it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed97e026",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_and_tables(pdf_path):\n",
    "\n",
    "    pdf_file = Path(pdf_path)\n",
    "    if not pdf_file.is_file() or pdf_file.suffix.lower() != \".pdf\":\n",
    "        raise FileNotFoundError(\"Provided file path is not a valid PDF.\")\n",
    "\n",
    "    doc = fitz.open(str(pdf_file))\n",
    "    result = []\n",
    "\n",
    "    # text extraction\n",
    "    for page_num, page in enumerate(doc, start = 1):\n",
    "        page_blocks = []\n",
    "\n",
    "        blocks = page.get_text(\"dict\")[\"blocks\"]\n",
    "        for block in blocks:\n",
    "            if block[\"type\"] == 0: # type 0 is text\n",
    "                text_content = \" \".join(\n",
    "                    span[\"text\"] for line in block[\"lines\"] for span in line[\"spans\"]\n",
    "                ).strip()\n",
    "                if text_content:\n",
    "                    y = block[\"bbox\"][1]\n",
    "                    page_blocks.append({\n",
    "                        \"type\": \"TEXT DATA\",\n",
    "                        \"y\": y,\n",
    "                        \"content\": text_content\n",
    "                    })\n",
    "\n",
    "        # table extraction\n",
    "        try:\n",
    "            tables = camelot.read_pdf(str(pdf_file), pages = str(page_num), flavor = 'lattice') # lattice flavor to extract tables\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to read tables on page {page_num}: {e}\")\n",
    "            tables = []\n",
    "\n",
    "        for table in tables:\n",
    "            table_data = table.data\n",
    "            bbox = table._bbox\n",
    "            y = float(bbox[1])\n",
    "            page_blocks.append({\n",
    "                \"type\": \"TABLE DATA\",\n",
    "                \"y\": y,\n",
    "                \"content\": table_data\n",
    "            })\n",
    "\n",
    "        page_blocks.sort(key = lambda b: b[\"y\"]) # sort contents on current page\n",
    "        result.extend(page_blocks) # append content to result list\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339473b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract texts and tables from the maual\n",
    "pre_result = extract_text_and_tables(\"manual.pdf\")\n",
    "pre_result[100:105] # few elements from the extracted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152acd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing 'fervi.com' background text\n",
    "result = []\n",
    "for res in pre_result:\n",
    "    if res['content'] != 'fervi.com':\n",
    "        result.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5075d163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample table data\n",
    "result[1173]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d1a0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list formatting by adding labels for text and table\n",
    "final = []\n",
    "for r in result:\n",
    "    s = f\"{r['type']}: {r['content']}\"\n",
    "    final.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8019de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# table data sample after flattening\n",
    "final[1173]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e498b6d4",
   "metadata": {},
   "source": [
    "It can be seen that the flattened version somewhat preserves the structure of the actual table by keeping each row inside a list. The LLM can hopefully understand this due to the presence of the label 'TABLE DATA' at the start."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d380b728",
   "metadata": {},
   "source": [
    "### 2. Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56cfbd30",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = [\" \".join(final[i:i+20]) for i in range(0, len(final), 20)] # be careful here\n",
    "print(f\"Number of chunks: {len(chunks)}\\n\")\n",
    "\n",
    "chunks[15] # sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a309f304",
   "metadata": {},
   "source": [
    "### 3. Creating BERT based vector embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0ade41",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "sem_embs = model.encode(chunks, convert_to_tensor = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53cde09",
   "metadata": {},
   "source": [
    "### 4. Pipeline to return indeces of top-K chunks that match with the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b50c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_query_pipeline(query, top_k = 20):\n",
    "     \n",
    "    device = sem_embs.device\n",
    "    query_embedding = model.encode(query, convert_to_tensor = True)\n",
    "    cosine_scores = util.cos_sim(query_embedding, sem_embs)[0]\n",
    "    top_indexes = np.argsort(cosine_scores.cpu().numpy())[::-1][:top_k]\n",
    "\n",
    "    return top_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63541914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = \"Summarize the manual.\" # 1\n",
    "# query = \"What are some general safety rules when using machine equipment?\" # 2\n",
    "# query = \"What does the manual say about unplugging the power cord of the machine from the power outlet?\"\" # 3\n",
    "# query = \"What are the several manual controls on the tool holder carriage?\" # 4\n",
    "query = \"Tell me about the lever for selection of longitudinal feeds.\" # 5\n",
    "# query = \"What does the document talk about regarding digital displays?\" # 6\n",
    "# query = \"What controls does the electric panel have?\" # 7\n",
    "# query = \"How to achieve balance when lifting the Lathe?\" # 8\n",
    "# query = \"Can I use the machine for turning non-ferrous materials?\" # 9\n",
    "# query = \"What should a grounding conductor be used for?\" # 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45b41d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all chunks from chunks retrived from both implementations\n",
    "\n",
    "bert_top_k_idx = bert_query_pipeline(query)\n",
    "final_idx = list(set(list(bert_top_k_idx)))\n",
    "\n",
    "top_context_chunks = [chunks[idx] for idx in final_idx] \n",
    "\n",
    "# get top embeddings directly from precomputed tensor\n",
    "sem_embs_final = torch.stack([sem_embs[idx] for idx in final_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb4a2e2",
   "metadata": {},
   "source": [
    "### 5. Function to get the final set of chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3495ef86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_final_idx(query):\n",
    "     \n",
    "    device = sem_embs.device\n",
    "    query_embedding = model.encode(query, convert_to_tensor = True)\n",
    "    cosine_scores = util.cos_sim(query_embedding, sem_embs_final)[0]\n",
    "    indexes = np.argsort(cosine_scores.cpu().numpy())[::-1]\n",
    "\n",
    "    return indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389429e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "best = bert_final_idx(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeeb07f3",
   "metadata": {},
   "source": [
    "### 6. Get final context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bddbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_string = \"\"\n",
    "for idx in best:\n",
    "    context_string += f\"{chunks[idx].strip()} (idx = {idx})\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae640592",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove unnecessary dots\n",
    "final_context_string = re.sub(r'\\.{2,}', '.', context_string)\n",
    "final_context_string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc03a1ae",
   "metadata": {},
   "source": [
    "### 7. Setting up LLM A that returns indexes of top-L chunks based on the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e6f52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_rank_contexts(query, final_context, top_l = 5):\n",
    "    context_string = \"\"\n",
    "    max_idx = len(final_context)\n",
    "    for i, ctx in enumerate(final_context):\n",
    "        context_string += f\"Context {i+1}:\\n{ctx.strip()}\\n\\n\"\n",
    "\n",
    "    return f\"\"\"\n",
    "        You are a highly skilled AI assistant that ranks technical contexts from a machinery operations and maintenance manual.\n",
    "\n",
    "        Your task is to rank the top-{top_l} most relevant contexts for answering the userâ€™s question, based solely on the provided content.\n",
    "\n",
    "        Each context ends with a tag in the format: **(idx = N)**, where N is an integer between 0 and {max_idx - 1}. Do not guess or invent index values.\n",
    "\n",
    "        **Instructions**:\n",
    "        - Carefully read all the context snippets.\n",
    "        - Identify the most relevant contexts that directly support answering the question.\n",
    "        - Return exactly {top_l} `idx` values, in descending order of relevance (most relevant first).\n",
    "        - Only include integers in the range 0 to {max_idx - 1}.\n",
    "        - Format your answer as a comma-separated list of integers with no extra text.\n",
    "\n",
    "        **Example**: 12, 4, 7, 1, 0\n",
    "\n",
    "        User Question:\n",
    "        {query}\n",
    "\n",
    "        Candidate Contexts:\n",
    "        {context_string}\n",
    "    \"\"\"\n",
    "\n",
    "def llm_a(query, final_context): # mistral might not be the best but no option\n",
    "    prompt = prompt_rank_contexts(query, final_context)\n",
    "\n",
    "    try:\n",
    "        response = ollama.generate(\n",
    "            model = 'mistral', \n",
    "            prompt = prompt,\n",
    "            stream = False\n",
    "        )\n",
    "        output = response['response'].strip()\n",
    "\n",
    "        # parse the returned string for integer idx values\n",
    "        ranked_indexes = [int(idx.strip()) for idx in output.split(\",\") if idx.strip().isdigit()]\n",
    "        return ranked_indexes[:5]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error using Ollama (Python client):\", str(e))\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca46f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get ids of top-L chunks\n",
    "top_ids = llm_a(query, final_context_string)\n",
    "print(f\"IDs of top-L chunks are: {top_ids}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b77d02",
   "metadata": {},
   "source": [
    "### 8. Get final context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76be8c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_context = ''\n",
    "for idx in top_ids:\n",
    "    final_context += chunks[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eacd2c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b86bc4c",
   "metadata": {},
   "source": [
    "\n",
    "### 9. Setting up LLM B for response generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5b69d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_b(prompt):\n",
    "    client = Groq(\n",
    "        api_key = os.getenv(\"GROQ_API_KEY\"),\n",
    "    )\n",
    "\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        model = \"llama-3.3-70b-versatile\",\n",
    "        # model = \"llama3-70b-8192\",\n",
    "        # model = \"mistral-saba-24b\",\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are an expert technical assistant specialized in interpreting operations and maintenance manuals for machinery.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ],\n",
    "        temperature = 0.5,\n",
    "        max_tokens = 5640,\n",
    "        top_p = 1,\n",
    "        stream = True,\n",
    "    )\n",
    "\n",
    "    for chunk in chat_completion:\n",
    "        content = chunk.choices[0].delta.content\n",
    "        if content:\n",
    "            print(content, end = '', flush = True)  # print to console without newline, flush immediately\n",
    "            time.sleep(0.01)  # optional tiny delay for typewriter effect\n",
    "    \n",
    "\n",
    "def prompt(query, context):\n",
    "    return f\"\"\"\n",
    "        You are an expert technical assistant specialized in interpreting operations and maintenance manuals for machinery.\n",
    "\n",
    "        Given the user question and the relevant extracted context from the manual:\n",
    "\n",
    "        - Provide a clear, precise, and factual answer to the question.\n",
    "        - Base your response strictly on the provided context; do not guess beyond it.\n",
    "        - If the context does not contain enough information, indicate that the answer is not available in the manual or that the context is not sufficient.\n",
    "        - Keep the answer professional, concise, and focused on practical instructions.\n",
    "        - Each section of the context begins with a tag: either 'TEXT DATA' or 'TABLE DATA'.\n",
    "        - 'TEXT DATA' represents plain, unstructured text. 'TABLE DATA' represents information extracted from a table and flattened into a list format.\n",
    "        - The 'TABLE DATA' is structured as a list of rows, where each row is a list containing the column values in order. The format is as follows: [[column 1 value, column 2 value, ...], [column 1 value, column 2 value, ...], ...]\n",
    "        \n",
    "        User Question:\n",
    "        {query}\n",
    "\n",
    "        Context from Manual:\n",
    "        {context}\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58263356",
   "metadata": {},
   "source": [
    "### 10. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6beb1679",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = prompt(query, final_context) # go to section number 4 to change query\n",
    "print(f\"QUERY: {query}\\n\")\n",
    "print('RESPONSE:')\n",
    "llm_b(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51962930",
   "metadata": {},
   "source": [
    "### 11. Assessing the final context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e7d904",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_context)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py310)",
   "language": "python",
   "name": "py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
