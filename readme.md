## Fusion Retrieval & Reranking based RAG
#### _An implementation and comparison of Fusion Retrieval based RAG, Reranking based RAG, and a Hybrid RAG that combines both._

---

1. **Fusion based RAG**<br>
This approach implements a retrieval-augmented generation (RAG) approach that combines semantic and keyword-based search methods for document querying. The process begins by extracting and chunking document content while preserving sequential context, then creates 2 sets of representations, one using BERT embeddings for semantic similarity and the other using BM25 for keyword matching. When a query is processed, it retrieves top-K chunks from both methods, performs a union operation to eliminate duplicates, and then re-scores this combined set using both BERT and BM25 approaches. The system calculates fusion scores by combining semantic and keyword relevance metrics, selects the top-L most relevant chunks (where L < K), and finally passes these chunks as context to a large language model along with the original query to generate a comprehensive, contextually-aware response. The fusion score weight can be adjusted by tweaking the α parameter. 

2. **Reranking based RAG**<br>
This approach implements a two-stage LLM-enhanced RAG system that leverages language models for chunk re-ranking after initial retrieval. The system extracts and chunks document content while maintaining sequential context, then creates BERT-based vector embeddings that capture semantic relationships through transformer encoder architecture. When processing a query, it first retrieves the top-K most similar chunks using cosine similarity on BERT embeddings, then introduces a novel re-ranking stage where an LLM evaluates these chunks alongside the original query to intelligently select the top-L most relevant pieces (where L < K). The selected chunks are then retrieved and passed as context to a second LLM along with the original query to generate the final response.

3. **Hybrid**<br>
This approach combines the fusion retrieval architecture by incorporating an additional LLM-based reranking stage, creating a three-tier chunk selection process. The approach begins with document extraction and chunking while preserving sequential context, then creates dual representations using BERT embeddings for semantic understanding and BM25 for keyword matching. During retrieval, the system first obtains top-K chunks from both methods, performs a union operation to eliminate duplicates, then re-scores this combined set using fusion metrics that combines semantic and keyword relevance to select the top-L chunks (where L < K). The balance between semantic and keyword relevance can be adjusted using the α parameter. The introduction of the first LLM that acts as a reranker, evaluating these top-L chunks alongside the query to identify the top-M most contextually appropriate chunks (where M < L). Finally, these carefully curated top-M chunks serve as context for a second LLM that generates the final response.
